{
 "agent": {
  "c_1": 1.25,
  "c_2": 19652,
  "cheating": false,
  "delta": 1.0,
  "dirichlet_alpha": 0.3,
  "dirichlet_eps": 0.25,
  "discount": 1,
  "epsilon": 0.0,
  "iterations": 50,
  "mdp_value": true,
  "n_search_threads": 1,
  "name": null,
  "nr_determinizations": 25,
  "player_func": false,
  "port": 9999,
  "temperature": 0.25,
  "terminal_func": false,
  "threads_to_use": 4,
  "type": "mu-zero-mcts",
  "virtual_loss": 1
 },
 "log": {
  "entity": "andrinburli",
  "group": "MuZero-MCTS-Experiment-1-small",
  "projectname": "jass-mu-zero-train"
 },
 "network": {
  "action_space_size": 43,
  "fc_hand_layers": [
   128
  ],
  "fc_player_layers": [
   128
  ],
  "fc_policy_layers": [
   128
  ],
  "fc_reward_layers": [
   128
  ],
  "fc_terminal_state_layers": [
   128
  ],
  "fc_value_layers": [
   128
  ],
  "fcn_blocks_dynamics": 0,
  "fcn_blocks_representation": 0,
  "feature_extractor": "cnn-full",
  "fully_connected": false,
  "num_blocks_dynamics": 2,
  "num_blocks_prediction": 0,
  "num_blocks_representation": 2,
  "num_channels": 128,
  "path": null,
  "players": 4,
  "reduced_channels_policy": 16,
  "reduced_channels_reward": 16,
  "reduced_channels_value": 16,
  "support_size": 200,
  "type": "resnet"
 },
 "optimization": {
  "adam_beta1": 0.9,
  "adam_beta2": 0.999,
  "adam_epsilon": 1e-07,
  "apa_n_games": 1,
  "batch_size": 512,
  "data_folder": "/app/results",
  "dldl": false,
  "grad_clip_norm": null,
  "hand_loss_weight": 0.001,
  "is_terminal_loss_weight": 1.0,
  "learning_rate": 0.0001,
  "learning_rate_init": null,
  "log_gradients": false,
  "log_inputs": false,
  "log_n_steps_ahead": 15,
  "log_visualisations": false,
  "mask_private": false,
  "mask_valid": false,
  "max_buffer_size": 800000,
  "max_samples_per_episode": 512,
  "max_steps_per_second": 100.0,
  "min_buffer_size": 800000,
  "min_non_zero_prob_samples": 1000,
  "optimizer": "adam",
  "player_loss_weight": 0.25,
  "policy_loss_weight": 1.0,
  "port": 8080,
  "reanalyse_fraction": 0.0,
  "restore_buffer_tree_from_file": false,
  "reward_entropy_weight": 0.0,
  "reward_loss_weight": 0.25,
  "reward_mse": false,
  "store_buffer": true,
  "store_model_weights_after": 10,
  "supervised_targets": false,
  "target_network_update": 100,
  "total_steps": 500000,
  "trajectory_length": 6,
  "updates_per_step": 10,
  "use_per": false,
  "value_based_per": false,
  "value_entropy_weight": 0.0,
  "value_loss_weight": 0.05,
  "value_mse": false,
  "value_td_5_step": false,
  "weight_decay": 1e-05
 },
 "timestamp": 4664621773
}